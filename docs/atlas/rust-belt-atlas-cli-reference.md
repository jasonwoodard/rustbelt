# Rust Belt Atlas CLI Reference

This reference captures the full contract for the `rustbelt-atlas` command line interface. It covers global options, each CLI subcommand, expected input schemas, output layouts, and minimal workflows for the prior, posterior, and blended scoring modes.

---

## Entry point & global switches

Run the Atlas CLI with either the `rustbelt-atlas` console script (after installing `packages/atlas-python`) or directly via `python -m atlas.cli`. Two flags are available at the top level before choosing a subcommand:

| Flag | Default | Description |
| --- | --- | --- |
| `--version` | _False_ | Print the installed `atlas-python` package version and exit. |
| `--explain` | _False_ | Materialise a sample scoring trace (JSON and CSV) in `--trace-dir` without running a full scoring pipeline. |
| `--trace-dir PATH` | `.` | Output directory for the lightweight sample generated by `--explain`. Created on demand. |

All other flags are scoped to a subcommand.

---

## Subcommand overview

| Subcommand | Purpose | Primary outputs | Schemas enforced |
| --- | --- | --- | --- |
| `score` | Run the prior, posterior, or blended scoring pipelines for a stores dataset. | Per-store scores plus optional traces/diagnostics. | [`schema/atlas/v1/score.schema.json`](../../schema/atlas/v1/score.schema.json) and [`schema/atlas/v1/trace.schema.json`](../../schema/atlas/v1/trace.schema.json) for optional traces. |
| `anchors` | Detect metro anchors via DBSCAN/HDBSCAN clustering. | Anchor metadata, optional store assignments, optional clustering metrics. | [`schema/atlas/v1/anchor.schema.json`](../../schema/atlas/v1/anchor.schema.json). |
| `subclusters` | Materialise a hierarchy of sub-clusters for a single anchor. | Sub-cluster table with lineage metadata. | [`schema/atlas/v1/cluster.schema.json`](../../schema/atlas/v1/cluster.schema.json). |

---

## Common recipes

The `fixtures/solver/atlas` directory contains copy/paste-ready commands that exercise the CLI end-to-end using the bundled dense urban datasets from `packages/atlas-python/src/atlas/fixtures/dense_urban`. Run them from the repository root; adjust extensions to `*.jsonl` to emit newline-delimited JSON instead of CSV.

### Blended scoring with traces

```bash
rustbelt-atlas score \
  --mode blended \
  --stores packages/atlas-python/src/atlas/fixtures/dense_urban/stores.csv \
  --affluence packages/atlas-python/src/atlas/fixtures/dense_urban/affluence.csv \
  --observations packages/atlas-python/src/atlas/fixtures/dense_urban/observations.csv \
  --output fixtures/solver/atlas/dense-urban-scores.csv \
  --lambda 0.5 \
  --trace-out fixtures/solver/atlas/dense-urban-trace.jsonl \
  --posterior-trace fixtures/solver/atlas/dense-urban-posterior-trace.csv
```

- Use `--trace-out` when you want a compact, stage-aware log of the prior/posterior/blend calculations. Keep the `.jsonl` extension (or add `--trace-format csv`) to control serialization.
- Supply `--posterior-trace` when you need the wide, posterior-only diagnostics; it also enables HTML/JSON/Parquet diagnostics sidecars next to the scored output unless `--no-diagnostics` is set.
- The score export contains per-store `Value`, `Yield`, and optional `Composite` columns; traces include the stage indicator and intermediate weights for deeper inspection.

### Anchor detection (CSV or JSONL)

```bash
rustbelt-atlas anchors \
  --stores packages/atlas-python/src/atlas/fixtures/dense_urban/stores.csv \
  --output fixtures/solver/atlas/dense-urban-anchors.csv \
  --store-assignments fixtures/solver/atlas/dense-urban-anchor-assignments.csv \
  --metrics fixtures/solver/atlas/dense-urban-anchor-metrics.json \
  --algorithm dbscan \
  --eps 0.03 \
  --min-samples 2 \
  --metric euclidean \
  --id-prefix metro-anchor
```

- Anchor exports list centroids and store counts, assignments map each `StoreId` to an anchor label, and the metrics JSON captures the DBSCAN tuning and cluster counts. Swap `*.csv` outputs for `*.jsonl` when you prefer newline-delimited JSON.

### Sub-cluster materialisation

```bash
rustbelt-atlas subclusters \
  --anchor-id metro-anchor-001 \
  --spec fixtures/solver/atlas/dense-urban-subcluster-spec.json \
  --output fixtures/solver/atlas/dense-urban-subclusters.jsonl \
  --id-prefix metro-anchor-001-sc
```

- The spec defines the target hierarchy; the CLI stamps each row with the anchor ID, lineage, and store roster. Emitting JSON lines makes it easy to concatenate multiple anchors, while a `.csv` extension produces a flat table.

---

## `score` command

### Flags

| Flag | Required | Default | Description |
| --- | --- | --- | --- |
| `--mode {prior-only,posterior-only,blended}` | No | `prior-only` | Selects the scoring pipeline. Posterior/blended modes require observations. |
| `--stores PATH` | Yes | – | Path to the stores dataset (CSV or JSON). |
| `--affluence PATH` | Conditionally | – | Affluence covariates. Required when the stores file lacks the normalised prior feature columns. |
| `--observations PATH` | Conditionally | – | Observation logs. Required in `posterior-only` and `blended` modes. |
| `--output PATH` | Yes | – | Destination for the scored stores table (CSV or JSON lines). Existing files are overwritten. |
| `--lambda WEIGHT` | No | – | Emits the λ-weighted `Composite` column: `λ·Value + (1−λ)·Yield`. Value must be between 0 and 1. |
| `--omega WEIGHT` | No | `0.5` | Blending weight for posterior vs prior scores. Constrained to `[0,1]`. Ignored in `prior-only` and `posterior-only` modes. |
| `--ecdf-window COLUMN` | No | – | Column used to segment posterior ECDF calculations (e.g., `Metro`). |
| `--ecdf-cache PATH` | No | – | Optional Parquet cache for ECDF references. |
| `--trace-out PATH` | No | – | Combined trace export for the executed stages. Format defaults to JSON lines. |
| `--trace-format {jsonl,csv}` | No | `jsonl` | Serialization format used when writing `--trace-out`. |
| `--posterior-trace PATH` | No | – | Posterior-only diagnostics export. Implies diagnostics sidecars. |
| `--posterior-trace-format {jsonl,csv}` | No | `csv` | Serialization format for `--posterior-trace`. |
| `--include-prior-trace` / `--no-include-prior-trace` | No | include | Toggle prior-stage rows in `--trace-out`. |
| `--include-posterior-trace` / `--no-include-posterior-trace` | No | include | Toggle posterior-stage rows in `--trace-out`. |
| `--include-blend-trace` / `--no-include-blend-trace` | No | include | Toggle blend-stage rows in `--trace-out`. |
| `--diagnostics-dir PATH` | No | Output directory | Overrides the diagnostics directory when posterior outputs are requested. Defaults to the scored output directory. |
| `--no-diagnostics` | No | diagnostics enabled | Disable diagnostics sidecars even when `--posterior-trace` is provided. |

#### Behaviour highlights

- **Normalised prior features**: `prior-only` and `blended` modes require `MedianIncomeNorm`, `Pct100kHHNorm`, and `PctRenterNorm`. Provide them in the stores file or pass `--affluence` with a dataset that can supply them. The CLI will surface an error if any of the normalised columns remain missing after preprocessing.
- **Affluence joins**: When `--affluence` is supplied, the CLI requires a `GeoId` column in the stores input. `_attach_affluence_features` joins the affluence file on `GeoId` and derives the normalised columns from any available `MedianIncome`, `Pct100kHH`, `PctRenter`, or `Turnover` series if they are not already present. Both datasets coerce `GeoId` to string to tolerate CSVs that inferred numeric types.
- **Posterior inputs**: `posterior-only` and `blended` modes refuse to run without `--observations`. The loader validates the payload against the observations schema before scoring.
- **Trace emission**: Trace exports respect the include/exclude toggles. Posterior traces and diagnostics are only written when the respective flags are set and data exists.
- **Omega defaults**: When not provided, `ω` defaults to `0.5`, evenly weighting prior and posterior scores in blended runs.

---

## `anchors` command

| Flag | Required | Default | Description |
| --- | --- | --- | --- |
| `--stores PATH` | Yes | – | Stores dataset (CSV or JSON). |
| `--output PATH` | Yes | – | Anchor metadata export (CSV or JSON lines). |
| `--store-assignments PATH` | No | – | Optional store-to-anchor assignment export. |
| `--metrics PATH` | No | – | Optional JSON dump of clustering diagnostics. |
| `--algorithm {dbscan,hdbscan}` | No | `dbscan` | Clustering algorithm. HDBSCAN unlocks the remaining tuning parameters. |
| `--eps FLOAT` | No | `0.5` | Neighbourhood radius (kilometres for haversine metric). |
| `--min-samples INT` | No | `5` | Minimum stores required to form a cluster. |
| `--metric {euclidean,manhattan,haversine}` | No | `haversine` | Distance metric. |
| `--min-cluster-size INT` | No | – | Minimum cluster size for HDBSCAN. |
| `--cluster-selection-epsilon FLOAT` | No | – | Cluster selection epsilon for HDBSCAN. |
| `--store-id-column NAME` | No | `StoreId` | Store identifier column used by the clustering engine. |
| `--lat-column NAME` | No | `Lat` | Latitude column in the stores dataset. |
| `--lon-column NAME` | No | `Lon` | Longitude column in the stores dataset. |
| `--metro-id VALUE` | No | – | Optional metro identifier carried into diagnostics. |
| `--id-prefix TEXT` | No | – | Prefix applied to generated anchor IDs. |

---

## `subclusters` command

| Flag | Required | Default | Description |
| --- | --- | --- | --- |
| `--anchor-id TEXT` | Yes | – | Anchor identifier that owns the sub-cluster hierarchy. |
| `--spec PATH` | Yes | – | JSON file describing the sub-cluster node specifications. |
| `--output PATH` | Yes | – | Destination for the materialised sub-clusters (CSV or JSON lines). |
| `--id-prefix TEXT` | No | Anchor ID | Override for the sub-cluster ID prefix. |

---

## Input dataset schemas

Atlas validates incoming datasets via the schemas in `packages/atlas-python/src/atlas/data/schema.py`. The tables below lift the canonical column lists and dtypes.

### Stores (`stores`)

| Column | Required | Dtype | Notes |
| --- | --- | --- | --- |
| `StoreId` | ✓ | string | Unique identifier for each store. Cast to string before scoring. |
| `Name` | ✓ | string | Human-readable store name. |
| `Type` | ✓ | string | Store category used by the prior. |
| `Lat` | ✓ | float64 | Latitude in decimal degrees. Duplicated to `Latitude` during scoring if absent. |
| `Lon` | ✓ | float64 | Longitude in decimal degrees. Duplicated to `Longitude` during scoring if absent. |
| `GeoId` | Conditional | string | Geo Identifier used to relate Stores to Affluence. Required when joining affluence data. |
| `ChainFlag` | – | string | Optional chain indicator. |
| `Notes` | – | string | Optional free-form notes. |
| `Zip` | – | string | Optional ZIP/ZCTA identifier. |


Additional expectations for prior/blended runs:

- Provide `GeoId` when you intend to join affluence data.
- Supply `MedianIncomeNorm`, `Pct100kHHNorm`, and `PctRenterNorm` either directly or via the affluence join.

### Affluence (`affluence`)

| Column | Required | Dtype | Notes |
| --- | --- | --- | --- |
| `GeoId` | ✓ | string | Join key shared with the stores dataset. |
| `MedianIncome` | ✓ | float64 | Source for `MedianIncomeNorm` when normalising. |
| `Pct100kHH` | ✓ | float64 | Source for `Pct100kHHNorm`. |
| `Education` | ✓ | float64 | Additional context preserved in diagnostics. |
| `HomeValue` | ✓ | float64 | Additional context preserved in diagnostics. |
| `Turnover` | ✓ | float64 | Source candidate for `PctRenterNorm`. |
| `Metro` | – | string | Optional metro label. |
| `County` | – | string | Optional county label. |

### Observations (`observations`)

| Column | Required | Dtype | Notes |
| --- | --- | --- | --- |
| `StoreId` | ✓ | string | Links observations back to the stores table. |
| `DateTime` | ✓ | string | Observation timestamp preserved as string for flexible parsing. |
| `DwellMin` | ✓ | float64 | Minutes spent in the store. |
| `PurchasedItems` | ✓ | Int64 | Count of purchased items. |
| `HaulLikert` | ✓ | float64 | 1–5 haul quality rating. |
| `ObserverId` | – | string | Optional observer identifier. |
| `Spend` | – | float64 | Optional spend amount. |
| `Notes` | – | string | Optional qualitative notes. |

#### Normalisation rules

`_attach_affluence_features` enforces the following logic when `--affluence` is provided:

1. Validate that the stores file contains a `GeoId` column so the join can succeed.
2. Merge `MedianIncome`, `Pct100kHH`, and `Turnover` (or their suffixed variants) onto the stores table.
3. For each required prior feature (`MedianIncomeNorm`, `Pct100kHHNorm`, `PctRenterNorm`), either coerce the existing column to numeric values or derive it by normalising the first available source candidate to the unit interval.
4. Raise an error if none of the candidate columns contain usable data.

As a result, prior and blended runs must carry either the normalised columns or a `GeoId` + affluence dataset that can supply them.

---

## Output schema reference

Atlas score, anchor, and sub-cluster exports are validated against the JSON Schemas under `schema/atlas/v1`. The tables below highlight key fields along with representative values from `packages/atlas-python/out/dense_blended.csv` where applicable.

### Scores (`score.schema.json`)

| Column | Type | Description | Example |
| --- | --- | --- | --- |
| `StoreId` | string | Store identifier. | `DU-001` |
| `Value` | number \| null | Final value score (0–5). | `4.5` |
| `Yield` | number \| null | Final yield score (0–5). | `3.0` |
| `Composite` | number \| null | λ-projected score when `--lambda` is provided. | `3.9` |
| `Omega` | number | Posterior weight used during blending (0–1). | `0.5` |
| `ValuePrior` | number \| null | Prior-stage value score. | `3.5` |
| `YieldPrior` | number \| null | Prior-stage yield score. | `3.2` |
| `CompositePrior` | number \| null | Prior-stage composite when available. | `3.38` |
| `ValuePosterior` | number \| null | Posterior-stage value score. | `4.5` |
| `YieldPosterior` | number \| null | Posterior-stage yield score. | `3.0` |
| `Theta` | number \| null | Posterior θ parameter backing the yield score. | `4.3892` |
| `Cred` | number \| null | Posterior credibility weight (0–1). | `0.051` |
| `Method` | string \| null | Posterior estimation strategy (`GLM`, `Hier`, `kNN`, …). | `Hier` |
| `ECDF_q` | number \| null | Posterior ECDF quantile associated with θ (0–1). | `0.5` |

### Anchors (`anchor.schema.json`)

| Column | Type | Description |
| --- | --- | --- |
| `anchor_id` | string | Stable identifier for the anchor cluster. |
| `cluster_label` | integer | Raw cluster label from DBSCAN/HDBSCAN (`-1` denotes noise). |
| `centroid_lat` | number | Centroid latitude (decimal degrees). |
| `centroid_lon` | number | Centroid longitude (decimal degrees). |
| `store_count` | integer | Number of stores assigned to the anchor. |
| `store_ids` | array[string] | Unique store identifiers linked to the anchor. |

### Sub-clusters (`cluster.schema.json`)

| Column | Type | Description |
| --- | --- | --- |
| `anchor_id` | string | Anchor identifier that owns the sub-cluster. |
| `subcluster_id` | string | Stable identifier for the sub-cluster lineage. |
| `parent_subcluster_id` | string \| null | Parent sub-cluster identifier when nested. |
| `lineage` | string | Dot-delimited ordinal lineage (e.g., `001.002`). |
| `depth` | integer | Hierarchy depth (1-indexed). |
| `store_count` | integer | Stores contained in the sub-cluster. |
| `store_ids` | array[string] | Unique store identifiers in the sub-cluster. |
| `centroid_lat` | number \| null | Optional centroid latitude (decimal degrees). |
| `centroid_lon` | number \| null | Optional centroid longitude (decimal degrees). |
| `metadata` | object | Additional attributes emitted by the hierarchy builder. |

---

## Mode-specific walkthroughs

Each scoring mode expects a minimal set of columns and exports. The commands below assume CSV inputs, but JSON line files are equally valid for `--stores`, `--affluence`, and `--observations`.

### Prior-only

**Minimum datasets**

- `stores.csv` containing the required columns listed above **plus** either:
  - Precomputed `MedianIncomeNorm`, `Pct100kHHNorm`, and `PctRenterNorm`, _or_
  - A `GeoId` column and a matching `affluence.csv` file.
- Optional affluence file (required if the normalised columns are missing).

**Example export from SQLite**

```bash
sqlite3 atlas.db "SELECT StoreId, Name, Type, Lat, Lon, GeoId, MedianIncomeNorm, Pct100kHHNorm, PctRenterNorm FROM stores;" > stores.csv
```

**Run the mode**

```bash
rustbelt-atlas score \
  --mode prior-only \
  --stores stores.csv \
  --affluence affluence.csv \
  --output out/prior-scores.csv \
  --lambda 0.6 \
  --trace-out out/prior-trace.jsonl
```

### Posterior-only

**Minimum datasets**

- `stores.csv` with the required columns (normalised prior features are optional).
- `observations.csv` with all required observation fields.

**Example export from SQLite**

```bash
sqlite3 atlas.db "SELECT StoreId, DateTime, DwellMin, PurchasedItems, HaulLikert FROM observations;" > observations.csv
```

**Run the mode**

```bash
rustbelt-atlas score \
  --mode posterior-only \
  --stores stores.csv \
  --observations observations.csv \
  --ecdf-window Metro \
  --output out/posterior-scores.csv \
  --posterior-trace out/posterior-trace.csv
```

### Blended

**Minimum datasets**

- `stores.csv` with the required columns **plus** the normalised prior features (directly or via `GeoId`).
- `affluence.csv` when the normalised features need to be derived.
- `observations.csv` for posterior adjustments.

**Example exports**

```bash
sqlite3 atlas.db "SELECT StoreId, Name, Type, Lat, Lon, GeoId FROM stores;" > stores.csv
sqlite3 atlas.db "SELECT GeoId, MedianIncome, Pct100kHH, Turnover FROM affluence;" > affluence.csv
sqlite3 atlas.db "SELECT StoreId, DateTime, DwellMin, PurchasedItems, HaulLikert FROM observations;" > observations.csv
```

**Run the mode**

```bash
rustbelt-atlas score \
  --mode blended \
  --stores stores.csv \
  --affluence affluence.csv \
  --observations observations.csv \
  --omega 0.5 \
  --lambda 0.6 \
  --output out/blended-scores.csv \
  --trace-out out/blended-trace.jsonl \
  --posterior-trace out/blended-posterior-trace.csv
```

Diagnostics sidecars (`atlas-diagnostics-v*.{html,json,parquet}`) will be written next to the posterior trace unless disabled.

---

## JSON workflows

To operate with JSON instead of CSV, supply newline-delimited JSON files and specify the same columns as keys. For example:

```bash
python -m atlas.cli score \
  --mode posterior-only \
  --stores stores.jsonl \
  --observations observations.jsonl \
  --output out/posterior.jsonl \
  --posterior-trace out/posterior-trace.jsonl \
  --posterior-trace-format jsonl
```

Ensure that each JSON object respects the same schemas described above.

---

## Further reading

- [Atlas docs index](./README.md)
- [Atlas user guide](./atlas-user-guide.md)
- [Atlas test plan](./atlas-test-plan.md)
- [Atlas technical plan](./rust-belt-atlas-tech-plan.md)

Use those guides alongside this reference to align CLI usage, validation strategies, and data expectations.
